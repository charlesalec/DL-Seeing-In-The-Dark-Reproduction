{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unet import *\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Device: cuda\n"
     ]
    }
   ],
   "source": [
    "Net = build_unet()\n",
    "# check if CUDA is available, and set it as the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"==> Device: {}\".format(device))\n",
    "\n",
    "# move model to the device\n",
    "Net.to(device)\n",
    "\n",
    "# define loss function\n",
    "# criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "####### HYPERPARAMETERS #######\n",
    "\n",
    "# learning rate = 10^-4\n",
    "lr = 0.0001\n",
    "\n",
    "# patch size\n",
    "patch_size = 512\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(Net.parameters(), lr=lr)\n",
    "\n",
    "# batch size\n",
    "batch_size = 1\n",
    "\n",
    "# define number of epochs\n",
    "n_epochs = 10\n",
    "\n",
    "# keep track of the best validation loss\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "# number of epochs to wait before stopping\n",
    "early_stopping = 5\n",
    "\n",
    "\n",
    "##### LOSS  #######\n",
    "\n",
    "# keep track of training and validation loss\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "# initialize the early_stopping object\n",
    "# early_stopping = EarlyStopping(patience=early_stopping, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, patch_size):\n",
    "    \"\"\"\n",
    "    Crop a random patch from the image\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = np.random.randint(0, w - patch_size)\n",
    "    y = np.random.randint(0, h - patch_size)\n",
    "    return img[y:y + patch_size, x:x + patch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00001_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_00_0.04s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 3, 512, 512])) that is different to the input size (torch.Size([1, 1, 512, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loss: 0.0892486423254013\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00002_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_00_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.06475134193897247\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00004_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_01_0.04s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.03906021639704704\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00009_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_01_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.0393085777759552\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00010_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_02_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.03978913277387619\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00012_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_03_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.024116944521665573\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00013_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_04_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.03507818281650543\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00014_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_05_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.11225885152816772\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00015_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_06_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.03906780108809471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:08<01:19,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00017_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_07_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.01960989646613598\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00001_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_00_0.04s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.03892054408788681\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00002_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_00_0.1s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.04740601032972336\n",
      "==> Label: /home/deeplearning/images/SonyImages/Sony/long/00004_00_10s.ARW\n",
      "==> Image: /home/deeplearning/images/SonyImages/Sony/short/00001_01_0.04s.ARW\n",
      "==> Image shape: (512, 512, 3)\n",
      "==> Label shape: (512, 512, 3)\n",
      "==> Loss: 0.015070416033267975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:11<01:44, 11.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/charles/Desktop/DL-Seeing-In-The-Dark-Reproduction/train.ipynb Cell 4\u001b[0m in \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/Desktop/DL-Seeing-In-The-Dark-Reproduction/train.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# convert the image and label to numpy arrays\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/Desktop/DL-Seeing-In-The-Dark-Reproduction/train.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mpostprocess(use_camera_wb\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, half_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, no_auto_bright\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, output_bps\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/charles/Desktop/DL-Seeing-In-The-Dark-Reproduction/train.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39;49mpostprocess(use_camera_wb\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, half_size\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, no_auto_bright\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_bps\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/Desktop/DL-Seeing-In-The-Dark-Reproduction/train.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# convert the image and label to float32 data type\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/charles/Desktop/DL-Seeing-In-The-Dark-Reproduction/train.ipynb#W4sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32(image \u001b[39m/\u001b[39m \u001b[39m65535.0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####### SAMPLE INPUT DATA #######\n",
    "labels_dir = '/home/deeplearning/images/SonyImages/Sony/long/'\n",
    "images_dir = '/home/deeplearning/images/SonyImages/Sony/short/'\n",
    "\n",
    "# labels_fns = glob.glob(labels_dir + '0*.ARW')\n",
    "# images_fns = glob.glob(images_dir + '0*.ARW')\n",
    "\n",
    "# label_ids = [int(os.path.basename(label_fn)[0:5]) for label_fn in labels_fns]\n",
    "# image_ids = [int(os.path.basename(image_fn)[0:5]) for image_fn in images_fns]\n",
    "# print(label_ids)\n",
    "# print(image_ids)\n",
    "\n",
    "# # # sort the images and labels\n",
    "# # labels_fns.sort()\n",
    "# # images_fns.sort()\n",
    "\n",
    "# # only train on 10 images, discard the rest\n",
    "# labels_fns = labels_fns[:10]\n",
    "# images_fns = images_fns[:10]\n",
    "\n",
    "# # # find all the input images from image_dir tha tcontain the same train_ids\n",
    "# images = [glob.glob(images_dir + '%05d_00*.ARW' % train_id) for train_id in image_ids]\n",
    "# labels = [glob.glob(labels_dir + '%05d_00*.ARW' % train_id) for train_id in label_ids]\n",
    "\n",
    "# # print them\n",
    "# print(labels)\n",
    "# print(images)\n",
    "\n",
    "\n",
    "# train_fns = glob.glob(labels_dir + '0*.ARW')\n",
    "# train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n",
    "\n",
    "# labels = train_ids[:10]\n",
    "\n",
    "# # find all the input images from image_dir tha tcontain the same train_ids \n",
    "# images = [glob.glob(images_dir + '%05d_00*.ARW' % train_id) for train_id in train_ids]\n",
    "# labels = [glob.glob(labels_dir + '%05d_00*.ARW' % train_id) for train_id in train_ids]\n",
    "\n",
    "# # sort them\n",
    "# train_ids.sort()\n",
    "# images.sort()\n",
    "\n",
    "# # print them\n",
    "# print(train_ids)\n",
    "# print(images)\n",
    "# \n",
    "\n",
    "\n",
    "images = glob.glob(images_dir + '0*.ARW')\n",
    "labels = glob.glob(labels_dir + '0*.ARW')\n",
    "\n",
    "# # sort the images and labels\n",
    "images.sort()\n",
    "labels.sort()\n",
    "\n",
    "# only train on 10 images, discard the rest\n",
    "images = images[:10]\n",
    "labels = labels[:10]\n",
    "\n",
    "epoch_loss = 0\n",
    "\n",
    "# loop over epochs with tqdm progress bar\n",
    "t = trange(n_epochs, leave=True)\n",
    "for epoch in t:\n",
    "    # initialize the training and validation loss for this epoch\n",
    "    train_loss_epoch = 0.0\n",
    "    valid_loss_epoch = 0.0\n",
    "\n",
    "    # set the model to training mode\n",
    "    Net.train()\n",
    "\n",
    "    # loop over the training data\n",
    "    for i in range(len(images)):\n",
    "        # load the first image and label\n",
    "        image = rawpy.imread(images[i])\n",
    "        label = rawpy.imread(labels[i])\n",
    "\n",
    "        # convert the image and label to numpy arrays\n",
    "        image = image.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "        label = label.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "\n",
    "        # convert the image and label to float32 data type\n",
    "        image = np.float32(image / 65535.0)\n",
    "        label = np.float32(label / 65535.0)\n",
    "\n",
    "        # crop the image and label to 512 x 512\n",
    "        image = random_crop(image, 512)\n",
    "        label = random_crop(label, 512)\n",
    "\n",
    "        # display the image and label\n",
    "        # plt.imshow(image)\n",
    "        # plt.show()\n",
    "        # plt.imshow(label)\n",
    "        # plt.show()\n",
    "\n",
    "        # display file name of label and input to check if they are correct\n",
    "        print(\"==> Label: {}\".format(labels[i]))\n",
    "        print(\"==> Image: {}\".format(images[i]))\n",
    "        print(\"==> Image shape: {}\".format(image.shape))\n",
    "        print(\"==> Label shape: {}\".format(label.shape))        \n",
    "\n",
    "        # convert the image and label to tensors\n",
    "        image = torch.from_numpy(np.expand_dims(np.transpose(image, (2, 0, 1)), axis=0))\n",
    "        label = torch.from_numpy(np.expand_dims(np.transpose(label, (2, 0, 1)), axis=0))\n",
    "\n",
    "        # move the image and label to the device\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        output = Net(image)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print the loss\n",
    "        print(\"==> Loss: {}\".format(loss.item()))\n",
    "        epoch_loss = loss.item()\n",
    "\n",
    "        # update the training loss\n",
    "        train_loss_epoch += loss.item()\n",
    "\n",
    "        # append loss to the list\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "# plot loss\n",
    "plt.plot(train_loss)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# plot "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
