{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import rawpy\n",
    "# import Dataset calss\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_raw_sony(raw):\n",
    "    # pack Bayer image to 4 channels\n",
    "    im = raw.raw_image_visible.astype(np.float32)\n",
    "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
    "\n",
    "    im = np.expand_dims(im, axis=2)\n",
    "    img_shape = im.shape\n",
    "    H = img_shape[0]\n",
    "    W = img_shape[1]\n",
    "\n",
    "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n",
    "# conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n",
    "# pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n",
    "\n",
    "# conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n",
    "# conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n",
    "# pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n",
    "\n",
    "# conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n",
    "# conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n",
    "# pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n",
    "\n",
    "# conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n",
    "# conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n",
    "# pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n",
    "\n",
    "# conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n",
    "# conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n",
    "\n",
    "# up6 = upsample_and_concat(conv5, conv4, 256, 512)\n",
    "# conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n",
    "# conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n",
    "\n",
    "# up7 = upsample_and_concat(conv6, conv3, 128, 256)\n",
    "# conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n",
    "# conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n",
    "\n",
    "# up8 = upsample_and_concat(conv7, conv2, 64, 128)\n",
    "# conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n",
    "# conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n",
    "\n",
    "# up9 = upsample_and_concat(conv8, conv1, 32, 64)\n",
    "# conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n",
    "# conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n",
    "\n",
    "# conv10 = slim.conv2d(conv9, 27, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n",
    "# out = tf.depth_to_space(conv10, 3)\n",
    "\n",
    "# def upsample_and_concat(x1, x2, output_channels, in_channels):\n",
    "#     pool_size = 2\n",
    "#     deconv_filter = nn.Parameter(torch.randn(output_channels, in_channels, pool_size, pool_size))\n",
    "#     deconv = nn.functional.conv_transpose2d(x1, deconv_filter, stride=pool_size)\n",
    "\n",
    "#     # Concatenate along the channel dimension\n",
    "#     deconv_output = torch.cat([deconv, x2], dim=1)\n",
    "\n",
    "#     return deconv_output\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.conv2d(in_channels=512, out_channels=32, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv2 = nn.conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv3 = nn.conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv4 = nn.conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv5 = nn.conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv6 = nn.conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv7 = nn.conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv8 = nn.conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv9 = nn.conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "#         self.conv10 = nn.conv2d(in_channels=32, out_channels=12, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "#         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "#         self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "\n",
    "#         self.up6 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "#         self.up7 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "#         self.up8 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "#         self.up9 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        \n",
    "#         self.lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "#         x = self.pool1.forward(x)\n",
    "\n",
    "#         x = self.conv2.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "#         x = self.pool2.forward(x)\n",
    "\n",
    "#         x = self.conv3.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "#         x = self.pool3.forward(x)\n",
    "\n",
    "#         x = self.conv4.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "#         x = self.pool4.forward(x)\n",
    "\n",
    "#         x = self.conv5.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "#         x = self.pool5.forward(x)\n",
    "\n",
    "#         x = self.up6.forward(x)\n",
    "#         x = self.conv6.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "\n",
    "#         x = self.up7.forward(x)\n",
    "#         x = self.conv7.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "\n",
    "#         x = self.up8.forward(x)\n",
    "#         x = self.conv8.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "\n",
    "#         x = self.up9.forward(x)\n",
    "#         x = self.conv9.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "\n",
    "#         x = self.conv10.forward(x)\n",
    "#         x = self.lrelu(x)\n",
    "        \n",
    "#         output = nn.functional.pixel_shuffle(x, 10)\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# From: https://github.com/nikhilroxtomar/Semantic-Segmentation-Architecture/blob/main/PyTorch/unet.py\n",
    "\n",
    "\"\"\" Convolutional block:\n",
    "    It follows a two 3x3 convolutional layer, each followed by a batch normalization and a relu activation.\n",
    "\"\"\"\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\"\"\" Encoder block:\n",
    "    It consists of an conv_block followed by a max pooling.\n",
    "    Here the number of filters doubles and the height and width half after every block.\n",
    "\"\"\"\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p\n",
    "\n",
    "\"\"\" Decoder block:\n",
    "    The decoder block begins with a transpose convolution, followed by a concatenation with the skip\n",
    "    connection from the encoder block. Next comes the conv_block.\n",
    "    Here the number filters decreases by half and the height and width doubles.\n",
    "\"\"\"\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rico\\.conda\\envs\\assignment3_cuda\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\Rico\\AppData\\Local\\Temp\\ipykernel_17952\\2350612420.py:80: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size = torch.Size([1, 4, 512, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 4, 512, 512] to have 3 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 145>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m gt \u001b[38;5;241m=\u001b[39m (gt_full)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput size = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss(out, gt)\n\u001b[0;32m    157\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\assignment3_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mbuild_unet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124;03m\"\"\" Encoder \"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     s1, p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     s2, p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2(p1)\n\u001b[0;32m     95\u001b[0m     s3, p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me3(p2)\n",
      "File \u001b[1;32m~\\.conda\\envs\\assignment3_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mencoder_block.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, p\n",
      "File \u001b[1;32m~\\.conda\\envs\\assignment3_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mconv_block.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\.conda\\envs\\assignment3_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\assignment3_cuda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\assignment3_cuda\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[1, 4, 512, 512] to have 3 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "input_dir = '/home/deeplearning/images/SonyImages/Sony/short/'\n",
    "gt_dir = '/home/deeplearning/images/SonyImages/Sony/long/'\n",
    "\n",
    "# input_dir = 'images/SonyImages/Sony/short/'\n",
    "# gt_dir = 'images/SonyImages/Sony/long/'\n",
    "\n",
    "ps = 512  # patch size for training\n",
    "save_freq = 500\n",
    "\n",
    "DEBUG = 0\n",
    "if DEBUG == 1:\n",
    "    save_freq = 2\n",
    "    train_ids = train_ids[0:5]\n",
    "\n",
    "def pack_raw(raw):\n",
    "    # pack Bayer image to 4 channels\n",
    "    im = raw.raw_image_visible.astype(np.float32)\n",
    "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
    "\n",
    "    im = np.expand_dims(im, axis=2)\n",
    "    img_shape = im.shape\n",
    "    H = img_shape[0]\n",
    "    W = img_shape[1]\n",
    "\n",
    "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "    return out\n",
    "\n",
    "def random_crop(input, gt, patch_size):\n",
    "    H = input.shape[0]\n",
    "    W = input.shape[1]\n",
    "\n",
    "    xx = np.random.randint(0, W - patch_size)\n",
    "    yy = np.random.randint(0, H - patch_size)\n",
    "\n",
    "    input = input[yy:yy + patch_size, xx:xx + patch_size, :]\n",
    "    gt = gt[yy * 2:yy * 2 + patch_size * 2, xx * 2:xx * 2 + patch_size * 2, :]\n",
    "\n",
    "    return input, gt\n",
    "\n",
    "def get_patch(input, gt, patch_size, ix=-1, iy=-1):\n",
    "    if ix == -1:\n",
    "        ix = np.random.randint(0, input.shape[1] - patch_size)\n",
    "    if iy == -1:\n",
    "        iy = np.random.randint(0, input.shape[0] - patch_size)\n",
    "    input = input[iy:iy + patch_size, ix:ix + patch_size, :]\n",
    "    gt = gt[iy * 2:iy * 2 + patch_size * 2, ix * 2:ix * 2 + patch_size * 2, :]\n",
    "\n",
    "    return input, gt\n",
    "\n",
    "def input_transform(*arg):\n",
    "    ret = []\n",
    "    for a in arg:\n",
    "        tmp = np.minimum(a / 16383, 1)\n",
    "        ret.append(tmp)\n",
    "    return ret\n",
    "\n",
    "def gt_transform(*arg):\n",
    "    ret = []\n",
    "    for a in arg:\n",
    "        tmp = np.minimum(a / 16383, 1)\n",
    "        ret.append(tmp)\n",
    "    return ret\n",
    "\n",
    "def get_training_data(batch_size=1):\n",
    "    train_fns = glob.glob(input_dir + '0*.ARW')\n",
    "    train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n",
    "\n",
    "    # get random sampple\n",
    "    train_ids = np.random.choice(train_ids, batch_size)\n",
    "\n",
    "\n",
    "    inputs = [None] * batch_size\n",
    "    gts = [None] * batch_size\n",
    "    for i in range(batch_size):\n",
    "        train_id = train_ids[i]\n",
    "        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n",
    "        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n",
    "        in_fn = os.path.basename(in_path)\n",
    "\n",
    "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n",
    "        gt_path = gt_files[0]\n",
    "        gt_fn = os.path.basename(gt_path)\n",
    "        gt_exposure = float(gt_fn[9:-5])\n",
    "        in_exposure = float(in_fn[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "\n",
    "        raw = rawpy.imread(in_path)\n",
    "        input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
    "\n",
    "        raw = rawpy.imread(gt_path)\n",
    "        gt_full = np.expand_dims(pack_raw(raw), axis=0)\n",
    "\n",
    "        H = input_full.shape[1]\n",
    "        W = input_full.shape[2]\n",
    "\n",
    "        xx = np.random.randint(0, W - ps)\n",
    "        yy = np.random.randint(0, H - ps)\n",
    "\n",
    "        input = input_full[:, yy:yy + ps, xx:xx + ps, :]\n",
    "        gt = gt_full[:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
    "\n",
    "        input = np.minimum(input, 1.0)\n",
    "\n",
    "        inputs[i] = np.transpose(input, (0, 3, 1, 2))\n",
    "        gts[i] = np.transpose(gt, (0, 3, 1, 2))\n",
    "\n",
    "    return np.concatenate(inputs, 0), np.concatenate(gts, 0)\n",
    "\n",
    "def get_test_data(input_path, gt_path, in_exposure, gt_exposure):\n",
    "    in_files = glob.glob(input_path + '0*.ARW')\n",
    "    in_path = in_files[0]\n",
    "    in_fn = os.path.basename(in_path)\n",
    "\n",
    "    gt_files = glob.glob(gt_path + '0*.ARW')\n",
    "    gt_path = gt_files[0]\n",
    "    gt_fn = os.path.basename(gt_path)\n",
    "\n",
    "    ratio = min(gt_exposure / in_exposure, 300)\n",
    "\n",
    "    raw = rawpy.imread(in_path)\n",
    "    input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
    "\n",
    "    raw = rawpy.imread(gt_path)\n",
    "    gt_full = np.expand_dims(pack_raw(raw), axis=0)\n",
    "\n",
    "    input_full = np.minimum(input_full, 1.0)\n",
    "\n",
    "    return np.transpose(input_full, (0, 3, 1, 2)), np.transpose(gt_full, (0, 3, 1, 2))\n",
    "\n",
    "Net = build_unet()\n",
    "\n",
    "# set up network optimizer, grad etc\n",
    "optimizer = torch.optim.Adam(Net.parameters(), lr=1e-4)\n",
    "mse_loss = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# load data\n",
    "input_full, gt_full = get_training_data(1)\n",
    "input_full = torch.from_numpy(input_full)\n",
    "gt_full = torch.from_numpy(gt_full)\n",
    "\n",
    "# train\n",
    "for epoch in range(5):\n",
    "    # begin training\n",
    "    Net.train()\n",
    "    optimizer.zero_grad()\n",
    "    input = (input_full)\n",
    "    gt = (gt_full)\n",
    "\n",
    "    print('input size = %s' % str(input.size()))\n",
    "    \n",
    "    out = Net(input)\n",
    "\n",
    "    loss = mse_loss(out, gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # Net.train()\n",
    "    # optimizer.zero_grad()\n",
    "    # input = Variable(input_full)\n",
    "    # gt = Variable(gt_full)\n",
    "\n",
    "    # print('input size = %s' % str(input.size()))\n",
    "\n",
    "    # out = Net(input)\n",
    "\n",
    "    # loss = mse_loss(out, gt)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # print('Epoch %d, loss = %f' % (epoch, loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
